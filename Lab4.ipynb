{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrLmfO3BTmuqe/qFBG5M7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoohw0317/2021knubootcamp/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Linear Regression**\n",
        "\n",
        "여러개의 정보를 통해서 하나의 결과를 도출해내는 학습법\n",
        "\n",
        "\n",
        "\n",
        "matmul()로 한번에 계산\n",
        "더 간결하고, x의의 길이가 바뀌어도 코드를 바꿀 필요가 없고 속도도 더 빠름.\n",
        "\n",
        "cost function의 수식은 비슷함.\n",
        "\n",
        "W : = W -a * d/dx a\n"
      ],
      "metadata": {
        "id": "4WdMJo38Dsyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMpIsnGXDqzs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 (여기가 simple regression과 달라짐)\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 모델 초기화 (여기도 약간 달라짐)\n",
        "W = torch.zeros((3,1), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1e-5)"
      ],
      "metadata": {
        "id": "4OLKLUj0nQZk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "  #H(x)를 계산)\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "  #Cost 계산, Cost는 점점 작아지도록 됨\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  #cost로 H(x)를 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'Epoch {epoch:4d}/{nb_epochs} hypothesis: {hypothesis.squeeze().detach()} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESVriFr9n-9O",
        "outputId": "a6b56c5d-76f0-4060-f724-cfd6f45aed44"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/20 hypothesis: tensor([152.8021, 183.6749, 180.9686, 197.0709, 140.1017]) Cost: 1.617638\n",
            "Epoch    1/20 hypothesis: tensor([152.8019, 183.6754, 180.9687, 197.0710, 140.1022]) Cost: 1.616936\n",
            "Epoch    2/20 hypothesis: tensor([152.8016, 183.6758, 180.9687, 197.0710, 140.1027]) Cost: 1.616214\n",
            "Epoch    3/20 hypothesis: tensor([152.8012, 183.6762, 180.9686, 197.0710, 140.1032]) Cost: 1.615513\n",
            "Epoch    4/20 hypothesis: tensor([152.8008, 183.6765, 180.9686, 197.0710, 140.1037]) Cost: 1.614808\n",
            "Epoch    5/20 hypothesis: tensor([152.8004, 183.6768, 180.9684, 197.0709, 140.1041]) Cost: 1.614109\n",
            "Epoch    6/20 hypothesis: tensor([152.8000, 183.6772, 180.9683, 197.0708, 140.1045]) Cost: 1.613405\n",
            "Epoch    7/20 hypothesis: tensor([152.7995, 183.6775, 180.9682, 197.0706, 140.1049]) Cost: 1.612695\n",
            "Epoch    8/20 hypothesis: tensor([152.7991, 183.6778, 180.9681, 197.0705, 140.1053]) Cost: 1.611981\n",
            "Epoch    9/20 hypothesis: tensor([152.7987, 183.6781, 180.9679, 197.0704, 140.1057]) Cost: 1.611289\n",
            "Epoch   10/20 hypothesis: tensor([152.7982, 183.6784, 180.9678, 197.0703, 140.1061]) Cost: 1.610577\n",
            "Epoch   11/20 hypothesis: tensor([152.7978, 183.6787, 180.9677, 197.0702, 140.1065]) Cost: 1.609871\n",
            "Epoch   12/20 hypothesis: tensor([152.7974, 183.6790, 180.9675, 197.0701, 140.1069]) Cost: 1.609167\n",
            "Epoch   13/20 hypothesis: tensor([152.7970, 183.6793, 180.9674, 197.0700, 140.1073]) Cost: 1.608482\n",
            "Epoch   14/20 hypothesis: tensor([152.7965, 183.6796, 180.9673, 197.0699, 140.1078]) Cost: 1.607761\n",
            "Epoch   15/20 hypothesis: tensor([152.7961, 183.6799, 180.9672, 197.0698, 140.1082]) Cost: 1.607080\n",
            "Epoch   16/20 hypothesis: tensor([152.7957, 183.6802, 180.9670, 197.0697, 140.1086]) Cost: 1.606368\n",
            "Epoch   17/20 hypothesis: tensor([152.7952, 183.6805, 180.9669, 197.0695, 140.1090]) Cost: 1.605665\n",
            "Epoch   18/20 hypothesis: tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094]) Cost: 1.604975\n",
            "Epoch   19/20 hypothesis: tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098]) Cost: 1.604280\n",
            "Epoch   20/20 hypothesis: tensor([152.7939, 183.6814, 180.9665, 197.0692, 140.1102]) Cost: 1.603572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Module 사용하기\n",
        "\n",
        "* nn.Module 을 상속해서 모델을 생성\n",
        "* nn.Linear (3, 1)\n",
        "  * 입력 차원: 3\n",
        "  * 출력 차원: 1\n",
        "* Hypothesis 계산은 forward()에서 처리됨\n",
        "* Gradient 계산은 PyTorch가 알아서 해줌 (backward() 통해서)\n",
        "\n"
      ],
      "metadata": {
        "id": "53J2edktozkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "TaHY2MNapWTA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F.mse_loss\n",
        "\n",
        "\n",
        "*   torch.nn.functional 에서 제공하는 loss function 사용\n",
        "*   쉽게 다른 loss와 교체 가능 (l1_loss, smooth_l1_loss 등등)\n",
        "\n"
      ],
      "metadata": {
        "id": "sOsxSHmEpqok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 (위의 코드와 변화 없음)\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 모델 초기화 - 매우 간결해짐\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer_new = optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "kXyADe8-p-OL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 50\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "  #H(x)를 계산 - model을 이용해서 계산하게 됨.\n",
        "  Hypothesis_new = model(x_train)\n",
        "\n",
        "  #Cost 계산, Cost는 점점 작아지도록 됨\n",
        "  cost_new = F.mse_loss(Hypothesis_new, y_train)\n",
        "\n",
        "  #cost로 H(x)를 개선\n",
        "  optimizer_new.zero_grad()\n",
        "  cost_new.backward()\n",
        "  optimizer_new.step()\n",
        "\n",
        "  print(f'Epoch {epoch:4d}/{nb_epochs} hypothesis: {Hypothesis_new.squeeze().detach()} Cost: {cost_new.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dzekUj2qgG5",
        "outputId": "428ecfcd-1e39-47a6-e372-7c68c0f1b4a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/50 hypothesis: tensor([-10.4987,  -6.2474,  -9.4404, -10.5937,  -2.8628]) Cost: 32507.056641\n",
            "Epoch    1/50 hypothesis: tensor([59.9116, 78.3796, 73.9445, 80.2103, 61.6860]) Cost: 10190.708008\n",
            "Epoch    2/50 hypothesis: tensor([ 99.3320, 125.7590, 120.6287, 131.0482,  97.8243]) Cost: 3195.718750\n",
            "Epoch    3/50 hypothesis: tensor([121.4023, 152.2848, 146.7656, 159.5105, 118.0566]) Cost: 1003.160339\n",
            "Epoch    4/50 hypothesis: tensor([133.7589, 167.1354, 161.3987, 175.4455, 129.3837]) Cost: 315.908936\n",
            "Epoch    5/50 hypothesis: tensor([140.6771, 175.4496, 169.5914, 184.3671, 135.7252]) Cost: 100.491470\n",
            "Epoch    6/50 hypothesis: tensor([144.5506, 180.1042, 174.1782, 189.3619, 139.2753]) Cost: 32.968658\n",
            "Epoch    7/50 hypothesis: tensor([146.7195, 182.7100, 176.7463, 192.1584, 141.2626]) Cost: 11.803207\n",
            "Epoch    8/50 hypothesis: tensor([147.9340, 184.1687, 178.1841, 193.7241, 142.3750]) Cost: 5.168192\n",
            "Epoch    9/50 hypothesis: tensor([148.6142, 184.9852, 178.9892, 194.6008, 142.9977]) Cost: 3.087708\n",
            "Epoch   10/50 hypothesis: tensor([148.9953, 185.4422, 179.4400, 195.0916, 143.3460]) Cost: 2.434845\n",
            "Epoch   11/50 hypothesis: tensor([149.2089, 185.6979, 179.6924, 195.3665, 143.5408]) Cost: 2.229497\n",
            "Epoch   12/50 hypothesis: tensor([149.3287, 185.8409, 179.8339, 195.5204, 143.6497]) Cost: 2.164384\n",
            "Epoch   13/50 hypothesis: tensor([149.3960, 185.9207, 179.9131, 195.6067, 143.7104]) Cost: 2.143245\n",
            "Epoch   14/50 hypothesis: tensor([149.4339, 185.9653, 179.9576, 195.6550, 143.7442]) Cost: 2.135882\n",
            "Epoch   15/50 hypothesis: tensor([149.4554, 185.9901, 179.9825, 195.6821, 143.7629]) Cost: 2.132846\n",
            "Epoch   16/50 hypothesis: tensor([149.4677, 186.0038, 179.9966, 195.6973, 143.7731]) Cost: 2.131172\n",
            "Epoch   17/50 hypothesis: tensor([149.4748, 186.0113, 180.0045, 195.7059, 143.7787]) Cost: 2.129907\n",
            "Epoch   18/50 hypothesis: tensor([149.4790, 186.0154, 180.0090, 195.7108, 143.7815]) Cost: 2.128767\n",
            "Epoch   19/50 hypothesis: tensor([149.4816, 186.0175, 180.0116, 195.7136, 143.7829]) Cost: 2.127685\n",
            "Epoch   20/50 hypothesis: tensor([149.4833, 186.0185, 180.0132, 195.7151, 143.7835]) Cost: 2.126619\n",
            "Epoch   21/50 hypothesis: tensor([149.4845, 186.0189, 180.0141, 195.7161, 143.7836]) Cost: 2.125561\n",
            "Epoch   22/50 hypothesis: tensor([149.4854, 186.0189, 180.0147, 195.7167, 143.7834]) Cost: 2.124503\n",
            "Epoch   23/50 hypothesis: tensor([149.4862, 186.0188, 180.0151, 195.7171, 143.7831]) Cost: 2.123418\n",
            "Epoch   24/50 hypothesis: tensor([149.4868, 186.0186, 180.0154, 195.7173, 143.7827]) Cost: 2.122358\n",
            "Epoch   25/50 hypothesis: tensor([149.4874, 186.0182, 180.0156, 195.7175, 143.7823]) Cost: 2.121294\n",
            "Epoch   26/50 hypothesis: tensor([149.4880, 186.0179, 180.0158, 195.7177, 143.7819]) Cost: 2.120231\n",
            "Epoch   27/50 hypothesis: tensor([149.4886, 186.0176, 180.0160, 195.7178, 143.7814]) Cost: 2.119176\n",
            "Epoch   28/50 hypothesis: tensor([149.4892, 186.0172, 180.0162, 195.7180, 143.7809]) Cost: 2.118102\n",
            "Epoch   29/50 hypothesis: tensor([149.4897, 186.0168, 180.0164, 195.7181, 143.7804]) Cost: 2.117045\n",
            "Epoch   30/50 hypothesis: tensor([149.4902, 186.0165, 180.0166, 195.7182, 143.7800]) Cost: 2.115995\n",
            "Epoch   31/50 hypothesis: tensor([149.4908, 186.0161, 180.0167, 195.7184, 143.7795]) Cost: 2.114950\n",
            "Epoch   32/50 hypothesis: tensor([149.4913, 186.0157, 180.0169, 195.7185, 143.7790]) Cost: 2.113873\n",
            "Epoch   33/50 hypothesis: tensor([149.4919, 186.0154, 180.0171, 195.7186, 143.7785]) Cost: 2.112807\n",
            "Epoch   34/50 hypothesis: tensor([149.4924, 186.0150, 180.0172, 195.7187, 143.7780]) Cost: 2.111749\n",
            "Epoch   35/50 hypothesis: tensor([149.4930, 186.0146, 180.0174, 195.7188, 143.7775]) Cost: 2.110707\n",
            "Epoch   36/50 hypothesis: tensor([149.4935, 186.0142, 180.0176, 195.7189, 143.7770]) Cost: 2.109642\n",
            "Epoch   37/50 hypothesis: tensor([149.4941, 186.0139, 180.0177, 195.7191, 143.7765]) Cost: 2.108586\n",
            "Epoch   38/50 hypothesis: tensor([149.4946, 186.0135, 180.0179, 195.7192, 143.7760]) Cost: 2.107527\n",
            "Epoch   39/50 hypothesis: tensor([149.4952, 186.0131, 180.0181, 195.7193, 143.7756]) Cost: 2.106471\n",
            "Epoch   40/50 hypothesis: tensor([149.4957, 186.0128, 180.0182, 195.7194, 143.7751]) Cost: 2.105430\n",
            "Epoch   41/50 hypothesis: tensor([149.4962, 186.0124, 180.0184, 195.7195, 143.7746]) Cost: 2.104366\n",
            "Epoch   42/50 hypothesis: tensor([149.4968, 186.0120, 180.0186, 195.7197, 143.7741]) Cost: 2.103333\n",
            "Epoch   43/50 hypothesis: tensor([149.4973, 186.0116, 180.0187, 195.7198, 143.7736]) Cost: 2.102271\n",
            "Epoch   44/50 hypothesis: tensor([149.4979, 186.0113, 180.0189, 195.7199, 143.7731]) Cost: 2.101230\n",
            "Epoch   45/50 hypothesis: tensor([149.4984, 186.0109, 180.0191, 195.7200, 143.7726]) Cost: 2.100167\n",
            "Epoch   46/50 hypothesis: tensor([149.4989, 186.0105, 180.0192, 195.7201, 143.7721]) Cost: 2.099115\n",
            "Epoch   47/50 hypothesis: tensor([149.4995, 186.0101, 180.0194, 195.7202, 143.7717]) Cost: 2.098072\n",
            "Epoch   48/50 hypothesis: tensor([149.5000, 186.0098, 180.0196, 195.7204, 143.7712]) Cost: 2.097010\n",
            "Epoch   49/50 hypothesis: tensor([149.5006, 186.0094, 180.0197, 195.7205, 143.7707]) Cost: 2.095970\n",
            "Epoch   50/50 hypothesis: tensor([149.5011, 186.0090, 180.0199, 195.7206, 143.7702]) Cost: 2.094917\n"
          ]
        }
      ]
    }
  ]
}